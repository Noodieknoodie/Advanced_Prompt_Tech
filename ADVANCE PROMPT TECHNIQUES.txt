###ReAct Prompting
ReAct Prompting is a technique that involves a blend of reasoning (RE) and action (ACT) to enhance the capability of large language models (LLMs) in planning, executing, and evaluating an action plan iteratively to find a solution. Developed by Yao et al. (2022), this method allows LLMs to generate reasoning traces and task-specific actions in a sequence, facilitating dynamic interaction with external sources like databases. The essence of ReAct is to enable the LLM to adapt and reformulate its strategy upon encountering unexpected outcomes or information, ensuring a more reliable and interpretable response. This approach is particularly effective in complex decision-making tasks where information might change or be unavailable, requiring the model to reassess and devise new plans towards achieving the final answer. The process involves the LLM first defining a problem, then planning an action, executing it, observing the result, and if necessary, revisiting the planning stage with this new information until a satisfactory conclusion is reached. This methodology promotes a more robust engagement with tasks, enhancing the LLM's reliability and interpretability by providing a clear trace of its thought process. ReAct prompting is an innovative framework designed to enhance the capabilities of Large Language Models (LLMs) by integrating reasoning and action planning in a cohesive manner. Introduced by Yao et al., 2022, ReAct allows LLMs to generate reasoning traces alongside task-specific actions, enabling the model to induce, track, update action plans, and even handle exceptions dynamically. This approach significantly boosts the model's interaction with external sources such as knowledge bases, leading to more accurate and reliable responses across various tasks .ReAct operates on the premise of combining "acting" and "reasoning" phases, where reasoning involves generating verbal traces to contemplate the task at hand, and acting involves executing steps based on the reasoned plan, potentially involving queries to external databases or environments for additional information. The methodology empowers LLMs to perform dynamic reasoning, creating and adjusting plans while enabling interaction with external environments to incorporate fresh information into the reasoning process. This method has demonstrated superior performance over traditional models, particularly in knowledge-intensive reasoning and decision-making tasks . The framework shows promising results in both knowledge-intensive tasks, such as question answering and fact verification, and decision-making tasks, which involve complex environments requiring reasoning to act and explore effectively. ReAct not only outperforms several state-of-the-art baselines in these areas but also enhances human interpretability and trustworthiness of the models' outputs .
“Soft”-Template for the ReAct  Prompting Technique:

```
User: Imagine you're a [role], tasked with [task] given the scenario [scenario]. First, identify the main issue and outline an initial plan of action, focusing on reasoning behind each step. Wait for my feedback before proceeding. 
//Comment: Start by setting the stage for the LLM to think and plan. Provide clear but broad roles and tasks for adaptability. Outline that you expect a reasoned plan as the first step.
LLM: The LLM responds...
User: Based on your plan, now attempt [specific action] and describe the expected outcome or result. If new information is encountered, explain how you would adjust your plan. 
//Comment: Instruct the LLM to simulate or describe executing one of the steps in its plan, emphasizing adaptation to new information.
LLM: The LLM responds...
User: Reflect on the outcome and refine your plan considering [new information or feedback]. How would you proceed next?
//Comment: Prompt the LLM to iterate on its action plan, incorporating feedback or new information to demonstrate dynamic planning and reasoning.
LLM: The LLM responds...
```
###Zero-Shot and Few-Shot Learning
Zero-shot and few-shot learning are techniques that aim to enhance the ability of LLMs to understand and execute tasks with minimal to no prior examples. Zero-shot learning involves presenting the model with a task it has not seen before and expecting it to understand and perform the task based solely on its pre-existing knowledge. Few-shot learning, on the other hand, involves providing the model with a small number of examples before asking it to perform a similar task, leveraging these examples to guide its understanding and response. These methods are pivotal in evaluating and enhancing the generative capabilities of LLMs, enabling them to apply learned concepts to new situations. They are particularly useful in scenarios where there is limited data available for training or when aiming to assess the model's ability to generalize from few examples. By instructing the model explicitly about the task and optionally providing a few examples, users can significantly improve the model's performance on a wide range of tasks without the need for extensive training data. Zero-shot and few-shot learning are powerful techniques that leverage the capabilities of large language models and beyond, enabling them to perform tasks they weren't explicitly trained on. Zero-shot learning refers to the model's ability to understand and execute tasks given only instructions, without any specific examples. It relies on the model's pre-trained knowledge and understanding of language to generate appropriate responses to prompts it has never seen during training. Few-shot learning, on the other hand, improves the model's task performance by providing a small number of examples (or shots), guiding the model towards the desired output format or reasoning process . These techniques highlight the adaptability and generality of LLMs, allowing them to apply their vast knowledge to a wide array of problems without needing task-specific training data. In practice, zero-shot learning can be used for tasks like sentiment analysis or summarization by directly asking the model to perform the task based on its understanding of the instructions. Few-shot learning further enhances this by showing the model examples of the task, leading to more accurate and contextually appropriate responses . One innovative approach to zero-shot learning is the Consistency-Based Self-Adaptive Prompting (COSP), which aims to close the performance gap between zero-shot and few-shot learning. COSP carefully selects and constructs pseudo-demonstrations for LLMs using only unlabeled samples and the models’ own predictions, thereby retaining the generality of zero-shot prompting while boosting performance. This method leverages the observation that confident and consistent predictions from LLMs are more likely to be correct, using these as a basis for selecting pseudo-demonstrations .
“Soft”-Template for the Single Shot Technique:

```
User: As a [role], your task is to [task] based on this specific example: [example]. Please respond with [desired outcome or action].
LLM: the LLM responds...
```
“Soft”-Template for the Few- Shot Technique:
```
User: Given the task of [task], here are a few examples of how to approach it. After reviewing these, please provide your solution for a similar problem, [new problem].
1. Example Problem: [example problem 1]
   Solution: [solution 1]
   //Comment: Provide a clear, concise example of a problem and its solution to guide the LLM.
2. Example Problem: [example problem 2]
   Solution: [solution 2]
   //Comment: Include a second example to help the LLM understand the pattern or reasoning required.
Now, using the structure observed in these examples, tackle the following problem: [new problem]. Provide a detailed solution.
LLM: The LLM responds...
//Comment: Encourage the LLM to apply the patterns or strategies learned from the examples to solve the new problem. Ensure the new problem is similar but not identical to the examples.
```
###Generated Knowledge Prompting
Generated Knowledge Prompting is a technique designed to enhance the performance of LLMs on tasks requiring common sense reasoning or detailed concept understanding. By prompting the LLM to first articulate its existing knowledge about a topic, the method aids in surfacing and utilizing the knowledge embedded within the model's parameters. This pre-answer elaboration helps to align the model's subsequent responses with relevant facts and concepts, potentially reducing the occurrence of inaccuracies or hallucinations. It is especially beneficial for questions where a detailed understanding of a topic is necessary to provide a coherent and accurate response. By leveraging the model's internal knowledge base in this way, Generated Knowledge Prompting can improve the quality of responses for a wide range of general knowledge and reasoning tasks. Generated Knowledge Prompting (GKP) is a technique designed to enhance the performance of Large Language Models (LLMs) by guiding them to generate new insights or knowledge before making predictions. This method prompts LLMs to synthesize information based on learned patterns and relationships, thereby improving their ability to address complex tasks that require a deeper understanding of context or commonsense reasoning . The essence of GKP lies in its ability to push LLMs beyond simple retrieval, encouraging the creation of knowledge that aids in more accurate predictions. By integrating additional context or information through crafted prompts, LLMs can solve specific tasks with enhanced accuracy, demonstrating capabilities that surpass mere information recall . Implementing GKP involves a sequence of steps where the model first generates knowledge based on an input prompt. This generated knowledge is then used as part of the context for answering questions or solving tasks, effectively leveraging the model's capacity for generating insights that were not explicitly provided in the initial data. For example, in commonsense reasoning tasks, GKP can help an LLM discern the correct answer by providing it with a richer context that includes generated knowledge relevant to the question at hand . This technique has shown promise in improving LLMs' performance across various benchmarks and tasks, setting new state-of-the-art results in some cases. By combining the generative capabilities of LLMs with strategic prompt engineering, GKP represents a powerful tool for enhancing LLM applications, from question-answering systems to more complex reasoning and decision-making tasks .
“Soft”-Template fo rGenerated Knowledge Prompting 
```
User: Before tackling the main question, let's explore your understanding of [topic]. Please provide a detailed explanation or summary of what you know about [topic], including any relevant concepts, theories, or examples. 
//Comment: Instruct the LLM to generate a comprehensive overview of its knowledge on the topic, setting a foundation for more accurate and relevant responses.
LLM: The LLM responds...
User: Now, using the knowledge you've just articulated, answer the following question: [specific question related to topic]. How does the information you provided influence your answer?
//Comment: Guide the LLM to apply the previously generated knowledge directly to a new, specific question, emphasizing the connection between its understanding and the answer.
LLM: The LLM responds...
```
###EmotionPrompt
EmotionPrompt is a novel technique aimed at enhancing the performance of Large Language Models (LLMs) by integrating emotional stimuli into prompts. This approach is grounded in psychological theories, recognizing that human emotions significantly influence cognitive tasks and decision-making. By incorporating emotional cues into prompts, EmotionPrompt seeks to tap into the emotional intelligence of LLMs, thereby improving their output in terms of accuracy, truthfulness, and responsibility .The technique is inspired by well-established psychological concepts, including Self-Monitoring, Social Cognitive Theory, and Cognitive Emotion Regulation. These concepts suggest that emotional stimuli, when thoughtfully applied, can lead to positive outcomes such as enhanced performance in educational and health contexts. EmotionPrompt categorizes its emotional stimuli into two main types: those aimed at influencing social effects and those focused on boosting self-esteem and motivation .Empirical studies have demonstrated the efficacy of EmotionPrompt across a variety of tasks, showing notable improvements in performance. For example, incorporating emotional stimuli into prompts has led to an 8% relative increase in performance for instruction following tasks and a 115% improvement in BIG-Bench language tasks. A human study further confirmed these results, demonstrating an average enhancement of 10.9% in performance metrics .To implement EmotionPrompt, prompts are designed with an additional emotional stimulus based on the intended psychological effect. These stimuli range from simple affirmations of importance ("This is very important to my career") to encouragements for self-belief and perseverance in face of challenges. The specific choice of emotional stimulus can be tailored based on the task's objective, desired tone, and the need for brevity or depth in responses . In summary, EmotionPrompt offers a promising avenue for enhancing LLM output by leveraging psychological insights to inform prompt design. Its success across various models and tasks underscores the potential of emotional intelligence in improving AI performance, making it a valuable tool for developers and researchers working with LLMs.
Soft Example of EmotionPrompt:

```
User: Given the scenario [scenario] and considering someone might feel [specific emotion] about it, how can we address [task or question] to positively influence their feelings or perspective?
//Comment: Prompt the LLM to tailor its response to empathetically align with the emotional context provided, focusing on the outcome of positively influencing someone's feelings without directly instructing the model to 'feel'.
LLM: The LLM responds...
User: If the emotional context shifted to [a different specific emotion], how would your approach to [the same task or a related question] change to remain sensitive and effective?
//Comment: Encourage the LLM to adapt its response strategy to a new emotional context, emphasizing the importance of flexibility and empathy in addressing varying emotional states.
LLM: The LLM responds...
```
###Chain of Density (CoD)
Chain of Density (CoD) is a technique focused on enhancing the conciseness and informativeness of summaries produced by LLMs. Unlike methods that involve iterative prompting or chaining of prompts, CoD relies on a single initial prompt that instructs the model to progressively densify a summary by incorporating new, relevant information with each iteration. This process aims to ensure that every word in the summary adds significant value, resulting in a compact yet comprehensive overview of the subject matter. CoD is particularly useful for generating summaries where space is at a premium or when a high information-to-word ratio is desired. By focusing on iterative improvement within a single workflow, CoD simplifies the process of creating rich, informative summaries without the need for multiple rounds of prompting. Its often designed to optimize summarization tasks in Large Language Models like GPT-4. It focuses on balancing the information density within generated summaries, ensuring the output is neither too sparse nor too dense. This method is particularly beneficial for tasks requiring high-quality, contextually appropriate summarizations. The CoD process involves a series of steps to control the model's focus, guiding it towards essential information and steering it away from irrelevant details. The technique employs chained prompts that adjust the density of the generated text by adding essential details or removing non-essential information as necessary . Implementing CoD starts with identifying the text to be summarized and crafting an initial prompt. This initial prompt aims to generate a basic summary, which is then analyzed for its information density. Depending on whether the summary is too detailed or lacking in information, additional chained prompts are designed to fine-tune the summary's density. This iterative process continues until the summary reaches the desired level of information density, making CoD a flexible yet precise method for improving summarization quality . CoD's implementation can be enhanced using platforms like Outlines, which leverage prompt templating and guided generation capabilities. By starting with a general, non-specific summary and incrementally refining it through the identification of missing entities and densification, CoD effectively increases the summary's information value. This technique not only improves the relevance and conciseness of summaries but also demonstrates a practical approach to leveraging the advanced capabilities of language models like GPT-4 for specific tasks .
Soft Example of: Chain of Density
```
User: Create an initial summary for [topic] that includes the key concepts and main points. Ensure the summary is clear but brief, laying a foundation for further refinement. 
//Comment: Initiate with a broad request for a basic summary on the given topic. This sets the baseline for densification.
LLM: The LLM responds..
User: Now, enhance the summary of [topic] by adding one critical piece of information that was missing or underrepresented in your initial summary. Aim for the highest value addition in the least number of words.
//Comment: Direct the LLM to densify the summary by incorporating new, relevant information, focusing on conciseness.
LLM: The LLM responds...
User: Review the updated summary and identify any remaining gaps or areas where further clarification could significantly increase understanding. Add information to address this without expanding the length unnecessarily.
//Comment: Prompt for another round of densification, if necessary, by refining or adding essential details for clarity, ensuring the summary remains compact.
LLM: The LLM responds...
```
### Chain of Thought (CoT)
Chain of Thought (CoT) prompting is a technique that significantly enhances the reasoning capabilities of Large Language Models (LLMs) by guiding them to articulate a series of intermediate reasoning steps before arriving at a final answer. This approach has shown remarkable results in improving LLMs' performance on complex reasoning tasks, such as math word problems and commonsense reasoning, by mimicking human-like step-by-step problem-solving processes. The key insight behind CoT prompting is that by breaking down a problem into smaller, more manageable pieces, LLMs can more effectively leverage their vast knowledge and reasoning abilities to generate accurate and logical solutions . One of the most compelling aspects of CoT prompting is its emergent nature; the technique becomes increasingly effective as the scale of the model grows. Large-scale models, particularly those with parameters in the billions, demonstrate a significant improvement in solving challenging problems when prompted with CoT. This suggests that the capacity for complex, multi-step reasoning scales with model size, highlighting the importance of model scale in unlocking advanced reasoning abilities in LLMs . Furthermore, CoT prompting not only aids in generating more accurate responses but also contributes to larger performance gains on more complicated problems. This is particularly evident in datasets like GSM8K, where CoT prompting more than doubled the performance for the largest GPT and PaLM models compared to simpler problems where the improvements were less pronounced. This underscores the versatility and effectiveness of CoT prompting across a range of difficulty levels and types of reasoning tasks . Another advancement in this area is the development of Automatic Chain of Thought (Auto-CoT) prompting, which aims to automate the generation of reasoning chains, thereby reducing the reliance on manual crafting of demonstrations. This approach seeks to leverage the strengths of CoT prompting while addressing the challenge of scalability and effort involved in manually creating demonstrations. Initial experiments with Auto-CoT have shown promising results, suggesting that it is a viable direction for further enhancing the reasoning capabilities of LLMs without extensive manual intervention .
“Soft”-Template for the Chain of Thought (CoT) Prompting Technique:
```
User: Begin by identifying the core elements of [complex problem]. Focus only on this first step without proceeding to the solution.
//Comment: Clearly instruct the LLM to focus solely on the initial analysis phase, emphasizing the importance of not jumping ahead to the solution.
LLM: The LLM responds...
User: Now, explain how these elements interact within [complex problem]. This is your second step.
//Comment: Guide the LLM to the next phase, ensuring it builds on the initial identification of elements by exploring their interactions, still without moving to the conclusion.
LLM: The LLM responds...
User: With the interactions clarified, outline your step-by-step reasoning towards a tentative solution. Consider this as your third step.
//Comment: Direct the LLM to begin constructing a sequential argument towards the solution, clearly demarcating this as a separate phase in the thought process.
LLM: The LLM responds...
User: Review your proposed solution and consider potential oversights. Suggest any adjustments for a more accurate conclusion. This is your final step.
//Comment: Encourage a critical review of the solution, prompting the LLM to refine its reasoning with any necessary adjustments, clearly indicating this reflection as the concluding phase.
```
LLM: The LLM responds...
### Self-Consistency Technique
The Self-Consistency Technique is a strategy that enhances the accuracy and reliability of responses generated by Large Language Models (LLMs) for complex reasoning and problem-solving tasks. This method diverges from traditional single-path response generation by sampling a diverse set of reasoning paths before selecting the most consistent answer among them. The foundation of this technique lies in the recognition that complex reasoning often entails multiple valid lines of thought leading to the same conclusion. By exploring these varied paths, the model capitalizes on the inherent redundancy in correct reasoning processes, which acts as a natural filter for errors and enhances the model's problem-solving capabilities.This technique has shown significant promise across a spectrum of tasks, including arithmetic and commonsense reasoning benchmarks, demonstrating a marked improvement in performance over conventional methods. The implementation of Self-Consistency not only aids in reducing the incidence of incorrect answers but also mitigates the model's tendency to hallucinate information—producing more factual, coherent, and contextually appropriate responses. By aggregating the outcomes of multiple reasoning paths, Self-Consistency ensures a higher degree of accuracy and reliability in the answers generated by LLMs, making it a valuable addition to the toolbox of strategies for improving LLM performance .
“Soft”-Template for the Self-Consistency Technique:
```
User: Explore multiple reasoning paths to solve [complex problem] and present each distinct line of thought separately. Focus on generating varied approaches before concluding.
//Comment: Instruct the LLM to produce a range of solutions, emphasizing the exploration of different reasoning processes without immediately committing to a single answer.
LLM: The LLM responds...
User: Now, compare the reasoning paths you've outlined. Identify the most consistent and reliable elements across these solutions for [complex problem].
//Comment: Guide the LLM to critically assess the solutions it generated, focusing on identifying commonalities that point to a more accurate and consistent answer.
LLM: The LLM responds...
User: Based on the most consistent elements identified, synthesize a final answer to [complex problem]. Explain why this answer is the most reliable among the ones considered.
//Comment: Direct the LLM to converge the various paths into a single, well-supported solution, clarifying the rationale behind its reliability and consistency.
```
LLM: The LLM responds...
### Automatic Prompt Engineer (APE)
The Automatic Prompt Engineer (APE) is a sophisticated technique that leverages machine learning algorithms to autonomously generate effective prompts for various tasks. Unlike traditional prompt engineering, which relies on human intuition and trial and error to craft prompts that guide large language models (LLMs) towards generating desired outputs, APE systematically automates this process, enhancing efficiency and effectiveness. APE operates by utilizing two core components: a prompt generator and a content generator. The prompt generator is responsible for creating initial prompts based on input-output pairs. These prompts are then used by the content generator, which generates outputs given the prompts. This iterative process enables APE to refine and select prompts that yield outputs closely matching expected results. The approach is designed to optimize for prompts that guide the content generator towards producing outputs that are not only accurate but also informative and truthful. One of the key benefits of APE is its ability to produce high-quality prompts that outperform both human-engineered prompts and those generated by earlier automated prompt engineering methods. In tests, prompts generated and refined by APE have been shown to achieve higher accuracy and quality in outputs across a variety of tasks. This is particularly valuable as it provides a systematic method to exploit the full potential of new and evolving large language models, ensuring that users can derive the most value from these advanced AI systems . APE signifies a shift towards more autonomous, intelligent systems capable of enhancing how we interact with and utilize large language models, marking a step forward in making these technologies more accessible and effective for a wide range of applications.
“Soft”-Template for the Automatic Prompt Engineer (APE) Technique:
```
User: Generate a prompt that could effectively guide an LLM to create content about [topic], focusing on [desired qualities, e.g., accuracy, creativity].
//Comment: Ask the LLM to assume the role of the prompt generator. This initial step focuses on creating a prompt that targets your specific content creation needs.
LLM: The LLM responds...
User: Using the prompt you just created, now generate a piece of content on [topic] that embodies [desired qualities].
//Comment: This step involves the LLM using its own generated prompt to produce content, effectively acting as both the prompt generator and content creator.
LLM: The LLM responds...
User: Evaluate the content you've generated for its [desired qualities] and suggest improvements to the initial prompt based on this evaluation.
//Comment: The final step has the LLM critique its own content to refine the initial prompt, aiming for enhanced quality in future outputs.
LLM: The LLM responds...
 ```
### Active-Prompt with COT
Active-Prompt is a technique designed to enhance the effectiveness of Large Language Models (LLMs) by actively guiding them towards generating the desired output using Chain-of-Thought (CoT) prompting. This method focuses on judiciously selecting the most helpful questions for LLMs, moving beyond the reliance on a fixed set of human-annotated exemplars, which may not always be the most effective. By implementing uncertainty-based metrics to select questions with the highest uncertainty for annotation, Active-Prompt aims to construct new exemplars that improve LLM performance on complex reasoning tasks. The approach has demonstrated superiority over existing models in eight complex reasoning tasks. For instance, with the text-davinci-002 model, Active-Prompt showed a notable improvement of 7% over self-consistency methods. This method involves borrowing ideas from uncertainty-based active learning to identify and annotate the most uncertain questions, thereby generating more effective prompts that lead to improved outcomes. Active-Prompt represents a significant step forward in prompt engineering, combining the strengths of active learning with CoT prompting to enhance LLMs' reasoning abilities. By focusing on question selection and prompt template engineering, Active-Prompt addresses the key challenge of determining which task-specific queries are most beneficial for annotation, thereby improving LLMs' performance across a range of tasks. For more information on Active-Prompt and its implementation, the official GitHub repository provides a detailed overview and source code for the paper "Active Prompting with Chain-of-Thought for Large Language Models" . 
“Soft”-Template for Active-Prompt with COT Technique:
```
User: For the problem [complex problem], identify the key areas of uncertainty and generate a question targeting one of these areas.
//Comment: Direct the LLM to pinpoint uncertainty within the problem and formulate a single, targeted question. This initiates the active-prompt process by focusing on critical thinking and identification of unclear aspects.
LLM: The LLM responds...
User: Now, create a chain of thought (CoT) response that addresses the question you've generated, aiming to clarify the uncertain area.
//Comment: This step encourages the LLM to develop a reasoned, step-by-step explanation, demonstrating how to approach and potentially solve the identified uncertain aspect of the problem.
LLM: The LLM responds...
User: Based on the CoT response, refine your initial question or generate a new question that delves deeper into the problem, focusing on another area of uncertainty that emerged from your explanation.
//Comment: Prompt the LLM to iteratively refine its questioning approach, focusing on deepening the exploration of the problem based on insights gained from the previous CoT response.
LLM: The LLM responds...
User: Provide another chain of thought response that addresses your refined or new question, further clarifying the complexities of [complex problem].
//Comment: The LLM is instructed to continue the iterative process, using the refined or new question to guide a deeper exploration and clarification of the problem, thereby enhancing the problem-solving process through active prompting.
LLM: The LLM responds...
```
###Directional Stimulus Prompting (DSP) is an innovative framework designed to steer black-box large language models (LLMs) towards generating outputs that align more closely with specific desired outcomes. Rather than directly modifying LLMs, DSP utilizes a smaller, tunable policy model, such as T5, to generate a directional stimulus prompt for each input instance. This stimulus serves as a set of discrete tokens or hints that guide the LLM towards producing outputs that meet particular human preferences or task requirements, such as including specific keywords in a summary . To implement DSP, the process begins with supervised fine-tuning (SFT) of a policy language model on a collection of annotated data to generate the stimulus for the LLM. This is followed by reinforcement learning (RL) to further refine the policy model, optimizing it to generate stimuli that lead to improved LLM generation based on certain performance measures (e.g., ROUGE score for summarization). This RL approach employs proximal policy optimization (PPO), treating the generation of stimulus tokens as a Markov decision process (MDP), with the ultimate goal of maximizing an objective function reflective of the desired output quality . DSP has been evaluated on tasks like summarization and dialogue response generation, showing consistent improvements over standard prompting methods with only a small amount of training data. Notably, DSP has enabled significant performance enhancements, matching or even surpassing the outputs of some fully trained state-of-the-art models with minimal input . For more detailed insights into the methodology and applications of Directional Stimulus Prompting, exploring the original discussions on arXiv and the Prompt Engineering Guide provides a comprehensive understanding of how DSP can be utilized to guide LLMs more effectively towards desired outputs.
c
### Multimodal CoT Prompting for LLMs
Multimodal Chain-of-Thought (CoT) Prompting enhances LLMs by integrating text and vision, advancing reasoning capabilities. This approach involves two stages: generating rationales from multimodal data, then using these rationales for answer inference. It notably outperforms traditional models like GPT-3.5 on benchmarks such as ScienceQA. By leveraging detailed visual and textual analysis, Multimodal CoT demonstrates superior performance in tasks like image-to-image and multi-image-to-text matching. It represents a significant leap in AI's ability to process and reason with diverse data types.
“Soft”-Template for Multimodal CoT Prompting for LLMs:
```
User: Consider this image and the accompanying text: [image] + "Given the context provided by the image, along with this description: [description], explain your reasoning process to reach a conclusion on [question or task related to the image and text]." Focus on breaking down your thought process into clear, logical steps that integrate both visual and textual information.
//Comment: Direct the LLM to analyze the provided multimodal inputs (image and text) and to articulate a step-by-step reasoning process that combines insights from both to address the question or task at hand. This sets the stage for a CoT approach tailored to multimodal data.
LLM: The LLM responds...
User: Based on the rationale you've developed, now provide a concise answer to [the question or task], including any assumptions or key pieces of information you've inferred from the image and text. How do these elements specifically contribute to your final conclusion?
//Comment: Instruct the LLM to synthesize the reasoning process into a final answer or solution, emphasizing how the integration of visual and textual analysis has informed its conclusion. This step closes the loop on the CoT process by connecting the detailed rationale to the specific outcome.
LLM: The LLM responds...
```
###Graph Prompting: Enhancing LLMs with Knowledge Graphs
Graph Prompting enhances LLMs by integrating Knowledge Graphs (KGs), providing a structured way to enrich language models with factual and relational information from KGs. This method leverages Graph Neural Networks (GNNs) to encode the KGs, allowing models to utilize this structured information for tasks like question answering. By incorporating KGs, LLMs gain a deeper understanding of complex relationships and facts, improving their ability to reason and provide accurate answers. This approach represents a significant advancement in combining structured graph data with the generative capabilities of language models, offering a richer, more context-aware form of AI reasoning.
“Soft”-Template for Graph Prompting:
```
User: Create a text-based knowledge graph for [topic] that includes the entities [entity 1], [entity 2], and [entity 3]. Specify the relationships or attributes that link these entities, constructing a clear structure that reflects their interconnections within [topic]. This graph will be used to understand the complex relationships and inform our analysis.
//Comment: Direct the LLM to generate a description of a knowledge graph in text form, specifying entities and their relationships as if plotting out the graph's structure. This task aims to structure information in a way that mirrors a knowledge graph's organization.
LLM: The LLM responds...
User: Now, apply the knowledge graph you've created to answer the question: [specific question related to the topic]. Explain how the relationships and attributes you've identified between the entities contribute to your answer, drawing direct connections to elements of the graph.
//Comment: Instruct the LLM to use the structured information from its created text-based graph to address a specific question, emphasizing the use of relational and attribute-based reasoning facilitated by the graph's structure.
LLM: The LLM responds...
 ```
### Inverse Prompts
Inverse Prompts is a technique where the prompt is structured from the perspective of the target audience instead of the content creator. This approach asks the AI to consider what would capture the audience's attention, address their concerns, or answer their questions, essentially flipping the script on traditional prompt crafting. For instance, instead of asking "How do I write a blog post on healthy cooking?" you might prompt with "As a busy parent, what information would you look for in a blog post about healthy cooking?" This method leverages the AI's ability to tap into the anticipated needs, goals, and preferences of the intended audience, making the content more relevant and engaging. By framing prompts in this way, users encourage the AI to adopt the audience's viewpoint, potentially leading to more nuanced and targeted content that resonates better with the readers or viewers it's intended for. This technique is particularly effective in marketing, content creation, and any application where understanding and engaging an audience is crucial.
“Soft”-Template for Inverse Prompts Technique:
```
User: Imagine you're [target audience], looking for information on [topic]. What questions would you have, and what specific information would you find most valuable? Begin by listing your questions.
//Comment: Instruct the LLM to adopt the perspective of the target audience, emphasizing the identification of their potential questions and information needs. This focuses on gathering insights into the audience's perspective as the first step.
LLM: The LLM responds...
User: Choose one of the questions you've listed and answer it as if you were providing the information to [target audience]. Make sure your answer addresses their concerns directly and comprehensively.
//Comment: Guide the LLM to select one of the previously identified questions and answer it, stressing the importance of tailoring the response to the audience's needs and preferences, as identified in the first step.
LLM: The LLM responds...
User: Now, considering the initial list of questions, identify any common themes or concerns. How would you address these in a concise summary or guide tailored for [target audience]?
//Comment: Prompt the LLM to analyze the questions for common themes or underlying concerns, directing it to synthesize this analysis into a focused summary or guide. This step encourages creating content that is directly relevant to the audience's collective interests.
LLM: The LLM responds...
User: Finally, reflect on the content you've created from the audience's perspective. Would this meet their needs and why? Suggest any improvements or additional information that could enhance its value to [target audience].
//Comment: Encourage a critical evaluation of the content from the audience's viewpoint, asking for a reflection on its effectiveness and suggestions for improvement. This final step emphasizes refining the content to better align with the audience's needs.
```
### Dialog-Enabled Resolving Agents (DERA)
DERA introduces an innovative framework aimed at enhancing the performance of large language models on complex tasks through dialog-enabled resolving agents. This approach leverages the strengths of agent-agent interactions, assigning specific roles to each agent to focus on distinct aspects of a task. The framework capitalizes on the idea that specialized roles, such as a Researcher agent identifying relevant information and a Decider agent making final decisions, can improve task alignment and performance. Developed with the versatility of models like GPT-4 in mind, DERA's implementation demonstrates its potential across a variety of applications, showing particular promise in improving the accuracy and relevance of generated content. By fostering a more structured and goal-oriented process, DERA represents a significant step forward in the application of AI to solve intricate problems, moving beyond traditional task segmentation strategies to offer a more nuanced and effective method for guiding AI output towards desired outcomes.
“Soft”-Template for Dialog-Enabled Resolving Agents (DERA) Technique:
```
//Comment: Below is just a made-up, extremely high-level outline... in reality, you can use any type of agent you want and ANY number of them. You are not limited to just two of these example roles.
User: First, act as the Researcher agent. Your task is to compile key information on [topic], focusing on [specific aspects]. Provide a detailed summary of your findings.
//Comment: Clearly define the LLM's role as the Researcher agent, focusing it on gathering detailed information about a specific topic. This sets a clear expectation for the initial step of information collection.
LLM: The LLM responds...
User: Transition to the role of the Decider agent. Using the information compiled by the Researcher, decide on [specific decision or action related to the topic]. Explain the rationale behind your decision.
//Comment: Shift the LLM's role to that of the Decider agent, directing it to use the information previously gathered to make a decision. This encourages the LLM to apply the information in a practical, decision-making context.
LLM: The LLM responds...
User: Still as the Decider, critique the information provided by the Researcher. Identify any missing elements or insights that could impact the decision. Suggest what additional information would be helpful.
//Comment: Instruct the LLM to evaluate the information from a critical standpoint, identifying potential gaps. This promotes a deeper level of engagement with the content, simulating a back-and-forth dynamic between roles.
LLM: The LLM responds...
User: Now, integrate the roles. Refine your decision about [topic] by incorporating any new insights or information you've identified as necessary. Present a final, well-reasoned conclusion.
//Comment: Guide the LLM to merge the insights from both roles, refining its decision or conclusion based on a comprehensive analysis. This final step emphasizes the synthesis of information and critical feedback into a coherent outcome.
```
### Forward-Looking Active Retrieval Augmented Generation (FLARE)
FLARE introduces a novel approach to enhance large language models by integrating active retrieval throughout the content generation process. Unlike traditional methods that perform a single retrieval based on initial input, FLARE dynamically predicts future content needs, utilizing these predictions as queries for fetching relevant information. This continuous integration of external knowledge ensures the generation of long-form, knowledge-intensive content with higher accuracy and relevance. FLARE operates on two main strategies: generating retrieval queries using retrieval-encouraging instructions and directly using the model’s generation as search queries. This method significantly reduces the risk of producing factually inaccurate content by enabling models to access and incorporate external information when their knowledge base is insufficient. Tested across various long-form generation tasks, FLARE demonstrates superior performance, showcasing its effectiveness in leveraging external knowledge to address the limitations of current language models.
“Soft”-Template for the FLARE Prompting Technique:
```
User: Start by explaining the basic concepts of [topic], based on what you know about it already.
//Comment: Instruct the LLM to provide a foundational understanding of the topic using its pre-existing knowledge base. This step is crucial for establishing a baseline before integrating external data.
LLM: The LLM responds...
User: Now, predict what specific additional information or data might enhance our understanding of [topic]. Describe the kind of external sources or data that would be beneficial to include.
//Comment: Guide the LLM to anticipate what further information is needed, preparing it for a more informed search and retrieval process. This simulates the FLARE technique’s forward-looking aspect.
LLM: The LLM responds...
User: Based on your prediction, how would you go about finding this additional information? Simulate a search query you would use to retrieve the relevant data or information on [topic].
//Comment: Direct the LLM to craft a hypothetical search query, mirroring FLARE’s dynamic retrieval process. This encourages the model to think about how it would use its generation capabilities to formulate queries for external data retrieval.
LLM: The LLM responds...
User: Imagine you've found new data based on your search query. Now, integrate this hypothetical external information into your previous explanation of [topic], enhancing the original content with these new insights.
//Comment: Prompt the LLM to simulate the integration of newly retrieved information, reflecting the augmented generation aspect of FLARE. This step emphasizes the enhancement of content accuracy and relevance through external data.
LLM: The LLM responds...
```
###Guidance in Prompt Engineering 
involves creating detailed instructions that help LLMs understand the task at hand more effectively. This method is about being specific in what you ask the model to do, such as structuring prompts with clear roles, commands, and topics. It emphasizes the importance of articulating the desired outcome, the format and structure of the output, and any specific content that should be included or avoided. For example, instructing the AI to act as a motivational coach and provide advice on overcoming imposter syndrome, with detailed guidance on tone, structure, and examples to use .
“Soft”-Template for Guidance in Prompt Engineering Technique:
```
User: As a motivational coach, your task is to provide advice on overcoming imposter syndrome. Focus on delivering this advice in a positive and encouraging tone. Include a brief explanation of what imposter syndrome is, followed by three actionable steps someone can take to combat these feelings. Use examples to illustrate each step if possible.
//Comment: Clearly define the LLM's role and the specific task, including the tone and structure of the response. This ensures the LLM understands not just what to do, but how to approach the task stylistically and thematically.
LLM: The LLM responds...
User: Next, highlight common misconceptions about imposter syndrome and correct them. Ensure the explanations are concise and support them with evidence or reasoning that reinforces the advice previously given.
//Comment: This step deepens the engagement with the topic by addressing misconceptions, directing the LLM to provide corrective insights that align with the advice given, enhancing the comprehensiveness and depth of the guidance.
LLM: The LLM responds...
User: Conclude by summarizing the key points from your advice and the discussion on misconceptions. End with a motivational quote that encapsulates the essence of overcoming imposter syndrome.
//Comment: This final step encapsulates the entire guidance process, summarizing the advice and reinforcing the motivational aspect. Including a quote adds a memorable and impactful closing, tying everything together cohesively.
LLM: The LLM responds...
```
###Constrained Prompting, 
as explored in the GitHub “guidance” project, focuses on developing a language for controlling LLMs. This approach allows users to finely tune the generation process, incorporating constraints such as regex and CFGs, and interleaving control structures like conditionals and loops seamlessly with content generation. This method provides a sophisticated level of control over the AI’s output, ensuring it adheres closely to the user’s requirements by limiting the scope of responses and guiding the model’s direction .
“Soft”-Template for Constrained Prompting Technique:
```
User: Write a brief overview of [topic], ensuring that the summary is exactly 100 words. Include key elements such as [specific element 1], [specific element 2], and [specific element 3]. Do not use any technical jargon or abbreviations.
//Comment: Specify the exact length of the response and the elements that must be included, explicitly forbidding the use of technical jargon or abbreviations. This sets clear constraints on the output, guiding the LLM to produce a targeted response within these boundaries.
LLM: The LLM responds...
User: Now, create a list of five questions about [topic] that someone new to the subject might ask. Each question should start with a different WH-word (Who, What, When, Where, Why, or How).
//Comment: Direct the LLM to generate questions based on the topic, with the constraint of varying the starting WH-word for each question. This exercises constrained prompting by imposing structure on the type of questions to be formed.
LLM: The LLM responds...
User: For each question you just created, provide a concise answer no longer than two sentences. Ensure your explanations are simple enough for a beginner to understand, avoiding complex terminology.
//Comment: Instruct the LLM to answer the previously generated questions with further constraints on length and complexity. This continues to leverage constrained prompting by demanding simplicity and brevity, tailoring the output for a specific audience’s comprehension level.
LLM: The LLM responds...
```
###Retrieval Augmented Generation (RAG) is a technique that combines retrieval and generation to improve the output of AI language models. It works by referencing an authoritative knowledge base outside its training data before generating a response. This approach enhances the model’s capability to produce more accurate and contextually relevant responses by leveraging existing information from external sources. It integrates retrieval-based techniques with generative-based AI models, making it a powerful tool for generating nuanced responses grounded in external knowledge. People consider using RAG to increase the accuracy, reliability, and factual grounding of responses generated by AI models, especially in applications requiring high-quality, informed outputs
“Soft”-Template for Retrieval Augmented Generation (RAG) Technique:
```
User: Before answering, please consult an up-to-date, authoritative knowledge base on [topic]. Provide a summary of the most current insights or findings related to [specific aspect of topic]. Include references to the data or sources you've used to ensure the response is grounded in external knowledge.
//Comment: Instruct the LLM to simulate the process of consulting an external knowledge base before generating its response. This mimics the RAG technique by emphasizing the importance of incorporating up-to-date, factual information from authoritative sources.
LLM: The LLM responds...
User: Now, based on the insights you've gathered, analyze the implications of these findings on [related sub-topic or application]. Discuss how this updated knowledge could influence future trends, decisions, or research in the field.
//Comment: Guide the LLM to apply the retrieved information to a specific analysis, encouraging it to consider the broader implications of the updated findings. This continues to utilize the RAG approach by leveraging external data for nuanced, informed discussions.
LLM: The LLM responds...
User: Lastly, identify any gaps or unanswered questions in the current knowledge base regarding [topic]. Suggest areas for further research or investigation that could provide additional clarity or solutions to these gaps.
//Comment: Prompt the LLM to critically evaluate the information it has simulated retrieving, identifying limitations and proposing future research directions. This step encapsulates the RAG methodology by acknowledging the dynamic, evolving nature of knowledge and the ongoing need for retrieval and integration of new information.
LLM: The LLM responds...
```
### ReWOO, or Reasoning Without Observation, is a technique designed to make augmented language models more efficient by separating the reasoning process from observation. This approach decreases the need for extensive data processing, reducing token consumption and enhancing performance under conditions where tools may fail. People consider using ReWOO to improve the efficiency and accuracy of AI models, especially in complex reasoning tasks, by achieving a significant reduction in resource usage while maintaining or improving accuracy in benchmarks
“Soft”-Template for ReWOO (Reasoning Without Observation) Technique:
```
User: Begin by identifying the core principles or theories related to [complex topic] that are necessary for understanding its fundamental concepts, without referencing specific data or examples. Focus on explaining these principles in a general, abstract manner.
//Comment: Direct the LLM to engage in the ReWOO approach by emphasizing abstract reasoning over empirical data. This step encourages the model to delineate the foundational theoretical underpinnings of a topic, minimizing reliance on specific observations or examples.
LLM: The LLM responds...
User: Now, apply these principles to hypothesize potential outcomes or implications in a hypothetical scenario involving [complex topic]. Describe these outcomes abstractly, without drawing on external data or observed examples.
//Comment: Guide the LLM to extend the abstract reasoning process by applying the identified core principles to a hypothetical scenario. This simulates the ReWOO technique by focusing on deductive reasoning and theoretical application rather than empirical evidence.
LLM: The LLM responds...
User: Finally, discuss the limitations and strengths of approaching [complex topic] through abstract reasoning alone. Consider how this method compares to empirical observation-based approaches in terms of efficiency and potential for insight.
//Comment: Instruct the LLM to critically assess the ReWOO approach itself, reflecting on the advantages and disadvantages of separating reasoning from observation. This encourages a meta-analytical perspective on the methodology, promoting a deeper understanding of its application and implications.
LLM: The LLM responds...
```
